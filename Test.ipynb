{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.698551Z",
     "start_time": "2024-10-17T11:49:59.685762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "115d560a9c9776ff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.729696Z",
     "start_time": "2024-10-17T11:49:59.715831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "29a723487c64e45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.761697Z",
     "start_time": "2024-10-17T11:49:59.751695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Updated Data Loading and Preprocessing\n",
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.images.append(os.path.join(class_path, img_name))\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "id": "d5a7fd633e17a91d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.792988Z",
     "start_time": "2024-10-17T11:49:59.779702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load and split the dataset\n",
    "def load_and_split_data(root_dir, train_ratio=0.8):\n",
    "    full_dataset = SignatureDataset(root_dir=root_dir, transform=transform)\n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset"
   ],
   "id": "19c61df1b0319d16",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.824362Z",
     "start_time": "2024-10-17T11:49:59.810755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VAE Implementation (unchanged)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ],
   "id": "9e64b394f42120d0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.855970Z",
     "start_time": "2024-10-17T11:49:59.841888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GAN Implementation (unchanged)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "d4abf419ce4b025",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.902729Z",
     "start_time": "2024-10-17T11:49:59.875756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training functions (slightly modified)\n",
    "def train_vae(vae, optimizer, train_loader, num_epochs=100):\n",
    "    vae.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch, _ in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(batch)\n",
    "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def train_gan(generator, discriminator, g_optimizer, d_optimizer, train_loader, num_epochs=100):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, _ in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            batch_size = batch.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            outputs = discriminator(batch)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, generator.latent_dim).to(device)\n",
    "            fake_images = generator(noise)\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ],
   "id": "8847865922d0b72e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:49:59.934816Z",
     "start_time": "2024-10-17T11:49:59.921713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generating New Signatures (updated for group-based generation)\n",
    "def generate_vae_signatures(vae, num_samples=10, group_idx=None):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, vae.latent_dim).to(device)\n",
    "        if group_idx is not None:\n",
    "            pass\n",
    "        samples = vae.decode(z)\n",
    "    return samples\n",
    "\n",
    "def generate_gan_signatures(generator, num_samples=10, group_idx=None):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, generator.latent_dim).to(device)\n",
    "        if group_idx is not None:\n",
    "            pass\n",
    "        samples = generator(noise)\n",
    "    return samples\n",
    "\n",
    "# Metrics and Evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, _ in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ],
   "id": "537ff17284949a2d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:55:19.678983Z",
     "start_time": "2024-10-17T11:49:59.951724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latent_dim = 100\n",
    "root_dir = 'Grouped_Output'\n",
    "    \n",
    "    # Load and split the data\n",
    "train_dataset, test_dataset = load_and_split_data(root_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # VAE training\n",
    "vae = VAE(latent_dim).to(device)\n",
    "vae_optimizer = optim.Adam(vae.parameters())\n",
    "train_vae(vae, vae_optimizer, train_loader)\n",
    "    "
   ],
   "id": "b0e81b9346112192",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 160422.1995\n",
      "Epoch 2, Loss: 159347.2236\n",
      "Epoch 3, Loss: 158751.6609\n",
      "Epoch 4, Loss: 158059.5696\n",
      "Epoch 5, Loss: 157183.5327\n",
      "Epoch 6, Loss: 156689.3054\n",
      "Epoch 7, Loss: 156461.6824\n",
      "Epoch 8, Loss: 156131.3247\n",
      "Epoch 9, Loss: 156011.7136\n",
      "Epoch 10, Loss: 155585.1194\n",
      "Epoch 11, Loss: 155100.2378\n",
      "Epoch 12, Loss: 152109.0845\n",
      "Epoch 13, Loss: 149654.5933\n",
      "Epoch 14, Loss: 148794.0710\n",
      "Epoch 15, Loss: 147444.4485\n",
      "Epoch 16, Loss: 145857.8625\n",
      "Epoch 17, Loss: 144927.4326\n",
      "Epoch 18, Loss: 144248.4912\n",
      "Epoch 19, Loss: 143396.3442\n",
      "Epoch 20, Loss: 143534.5188\n",
      "Epoch 21, Loss: 142582.5330\n",
      "Epoch 22, Loss: 143233.6924\n",
      "Epoch 23, Loss: 143123.6804\n",
      "Epoch 24, Loss: 142641.7344\n",
      "Epoch 25, Loss: 143147.5879\n",
      "Epoch 26, Loss: 142453.2803\n",
      "Epoch 27, Loss: 142372.2751\n",
      "Epoch 28, Loss: 142048.8809\n",
      "Epoch 29, Loss: 142579.9226\n",
      "Epoch 30, Loss: 142576.4507\n",
      "Epoch 31, Loss: 142346.5527\n",
      "Epoch 32, Loss: 141941.6963\n",
      "Epoch 33, Loss: 142172.1826\n",
      "Epoch 34, Loss: 141283.3743\n",
      "Epoch 35, Loss: 141989.0884\n",
      "Epoch 36, Loss: 141204.7395\n",
      "Epoch 37, Loss: 141518.9128\n",
      "Epoch 38, Loss: 140576.5310\n",
      "Epoch 39, Loss: 142109.5884\n",
      "Epoch 40, Loss: 141358.9844\n",
      "Epoch 41, Loss: 141704.9016\n",
      "Epoch 42, Loss: 141632.4028\n",
      "Epoch 43, Loss: 141735.3660\n",
      "Epoch 44, Loss: 141231.8154\n",
      "Epoch 45, Loss: 141154.3464\n",
      "Epoch 46, Loss: 140823.1069\n",
      "Epoch 47, Loss: 142175.7539\n",
      "Epoch 48, Loss: 140945.9834\n",
      "Epoch 49, Loss: 141545.7952\n",
      "Epoch 50, Loss: 141612.4658\n",
      "Epoch 51, Loss: 141107.3257\n",
      "Epoch 52, Loss: 141334.1750\n",
      "Epoch 53, Loss: 141447.9836\n",
      "Epoch 54, Loss: 141375.4880\n",
      "Epoch 55, Loss: 141003.2107\n",
      "Epoch 56, Loss: 141275.3154\n",
      "Epoch 57, Loss: 141101.3433\n",
      "Epoch 58, Loss: 141842.4963\n",
      "Epoch 59, Loss: 140918.9797\n",
      "Epoch 60, Loss: 140508.7832\n",
      "Epoch 61, Loss: 140570.2759\n",
      "Epoch 62, Loss: 140681.6345\n",
      "Epoch 63, Loss: 141173.6633\n",
      "Epoch 64, Loss: 141134.7622\n",
      "Epoch 65, Loss: 141014.0015\n",
      "Epoch 66, Loss: 141158.3438\n",
      "Epoch 67, Loss: 140981.4561\n",
      "Epoch 68, Loss: 141056.4612\n",
      "Epoch 69, Loss: 140652.7566\n",
      "Epoch 70, Loss: 140425.5947\n",
      "Epoch 71, Loss: 140695.7131\n",
      "Epoch 72, Loss: 141113.7219\n",
      "Epoch 73, Loss: 140652.2476\n",
      "Epoch 74, Loss: 139643.4302\n",
      "Epoch 75, Loss: 140997.8865\n",
      "Epoch 76, Loss: 140436.4766\n",
      "Epoch 77, Loss: 140883.2188\n",
      "Epoch 78, Loss: 140402.2593\n",
      "Epoch 79, Loss: 140135.5364\n",
      "Epoch 80, Loss: 140914.3086\n",
      "Epoch 81, Loss: 140211.9216\n",
      "Epoch 82, Loss: 140268.2517\n",
      "Epoch 83, Loss: 139749.6267\n",
      "Epoch 84, Loss: 140109.6907\n",
      "Epoch 85, Loss: 140170.2043\n",
      "Epoch 86, Loss: 140460.9919\n",
      "Epoch 87, Loss: 140450.9885\n",
      "Epoch 88, Loss: 140738.1016\n",
      "Epoch 89, Loss: 140082.1211\n",
      "Epoch 90, Loss: 140342.7676\n",
      "Epoch 91, Loss: 140187.0183\n",
      "Epoch 92, Loss: 139665.1921\n",
      "Epoch 93, Loss: 139952.5840\n",
      "Epoch 94, Loss: 140436.9041\n",
      "Epoch 95, Loss: 140016.4355\n",
      "Epoch 96, Loss: 139852.9009\n",
      "Epoch 97, Loss: 140003.8191\n",
      "Epoch 98, Loss: 140313.8203\n",
      "Epoch 99, Loss: 141095.2981\n",
      "Epoch 100, Loss: 140140.1135\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:00:03.469265Z",
     "start_time": "2024-10-17T11:55:19.712418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # GAN training\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "train_gan(generator, discriminator, g_optimizer, d_optimizer, train_loader)"
   ],
   "id": "a6bd08006372ea10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], d_loss: 0.2056, g_loss: 3.4669\n",
      "Epoch [2/100], d_loss: 0.0913, g_loss: 4.7284\n",
      "Epoch [3/100], d_loss: 0.0592, g_loss: 5.1191\n",
      "Epoch [4/100], d_loss: 0.0405, g_loss: 5.5071\n",
      "Epoch [5/100], d_loss: 0.0310, g_loss: 5.8096\n",
      "Epoch [6/100], d_loss: 0.0158, g_loss: 6.5883\n",
      "Epoch [7/100], d_loss: 0.0286, g_loss: 7.3582\n",
      "Epoch [8/100], d_loss: 2.4944, g_loss: 13.1855\n",
      "Epoch [9/100], d_loss: 0.0994, g_loss: 4.1361\n",
      "Epoch [10/100], d_loss: 0.0780, g_loss: 4.7233\n",
      "Epoch [11/100], d_loss: 0.8668, g_loss: 7.9659\n",
      "Epoch [12/100], d_loss: 0.4698, g_loss: 6.0076\n",
      "Epoch [13/100], d_loss: 0.2992, g_loss: 3.2368\n",
      "Epoch [14/100], d_loss: 0.1976, g_loss: 8.5597\n",
      "Epoch [15/100], d_loss: 0.0780, g_loss: 5.0591\n",
      "Epoch [16/100], d_loss: 0.0683, g_loss: 5.4908\n",
      "Epoch [17/100], d_loss: 0.1305, g_loss: 4.6688\n",
      "Epoch [18/100], d_loss: 0.0686, g_loss: 5.7530\n",
      "Epoch [19/100], d_loss: 0.2793, g_loss: 9.5638\n",
      "Epoch [20/100], d_loss: 0.0770, g_loss: 6.0474\n",
      "Epoch [21/100], d_loss: 0.0182, g_loss: 5.3337\n",
      "Epoch [22/100], d_loss: 0.0304, g_loss: 5.4975\n",
      "Epoch [23/100], d_loss: 0.0287, g_loss: 6.9334\n",
      "Epoch [24/100], d_loss: 0.0371, g_loss: 6.0643\n",
      "Epoch [25/100], d_loss: 0.0168, g_loss: 6.0013\n",
      "Epoch [26/100], d_loss: 0.0415, g_loss: 5.4863\n",
      "Epoch [27/100], d_loss: 0.0573, g_loss: 6.1377\n",
      "Epoch [28/100], d_loss: 0.0521, g_loss: 5.8485\n",
      "Epoch [29/100], d_loss: 0.0677, g_loss: 7.3020\n",
      "Epoch [30/100], d_loss: 0.0095, g_loss: 5.2614\n",
      "Epoch [31/100], d_loss: 0.0123, g_loss: 6.5196\n",
      "Epoch [32/100], d_loss: 0.0326, g_loss: 5.3615\n",
      "Epoch [33/100], d_loss: 0.0562, g_loss: 5.0625\n",
      "Epoch [34/100], d_loss: 0.0277, g_loss: 5.5774\n",
      "Epoch [35/100], d_loss: 0.0217, g_loss: 5.5789\n",
      "Epoch [36/100], d_loss: 0.0406, g_loss: 6.3611\n",
      "Epoch [37/100], d_loss: 0.0480, g_loss: 7.2576\n",
      "Epoch [38/100], d_loss: 0.1220, g_loss: 1.6250\n",
      "Epoch [39/100], d_loss: 0.0989, g_loss: 4.9728\n",
      "Epoch [40/100], d_loss: 0.1500, g_loss: 4.3359\n",
      "Epoch [41/100], d_loss: 0.0965, g_loss: 6.4165\n",
      "Epoch [42/100], d_loss: 0.2213, g_loss: 4.0249\n",
      "Epoch [43/100], d_loss: 0.0852, g_loss: 5.1281\n",
      "Epoch [44/100], d_loss: 0.0438, g_loss: 5.2439\n",
      "Epoch [45/100], d_loss: 0.0704, g_loss: 4.9491\n",
      "Epoch [46/100], d_loss: 0.0327, g_loss: 6.2119\n",
      "Epoch [47/100], d_loss: 0.0485, g_loss: 5.0198\n",
      "Epoch [48/100], d_loss: 0.0604, g_loss: 4.7127\n",
      "Epoch [49/100], d_loss: 0.0625, g_loss: 4.6369\n",
      "Epoch [50/100], d_loss: 0.0523, g_loss: 4.4334\n",
      "Epoch [51/100], d_loss: 0.1938, g_loss: 4.0640\n",
      "Epoch [52/100], d_loss: 0.0231, g_loss: 5.4658\n",
      "Epoch [53/100], d_loss: 0.0267, g_loss: 5.4162\n",
      "Epoch [54/100], d_loss: 0.0171, g_loss: 5.4683\n",
      "Epoch [55/100], d_loss: 0.0484, g_loss: 6.0370\n",
      "Epoch [56/100], d_loss: 0.0348, g_loss: 5.9254\n",
      "Epoch [57/100], d_loss: 0.0208, g_loss: 6.4652\n",
      "Epoch [58/100], d_loss: 0.0151, g_loss: 6.0068\n",
      "Epoch [59/100], d_loss: 0.0122, g_loss: 5.9508\n",
      "Epoch [60/100], d_loss: 0.0108, g_loss: 5.9270\n",
      "Epoch [61/100], d_loss: 0.0146, g_loss: 6.3987\n",
      "Epoch [62/100], d_loss: 0.0195, g_loss: 6.1501\n",
      "Epoch [63/100], d_loss: 0.0127, g_loss: 6.6258\n",
      "Epoch [64/100], d_loss: 0.0809, g_loss: 5.4196\n",
      "Epoch [65/100], d_loss: 0.0121, g_loss: 6.0892\n",
      "Epoch [66/100], d_loss: 0.0062, g_loss: 6.0759\n",
      "Epoch [67/100], d_loss: 0.0206, g_loss: 5.9336\n",
      "Epoch [68/100], d_loss: 0.0099, g_loss: 6.5760\n",
      "Epoch [69/100], d_loss: 0.0288, g_loss: 5.7903\n",
      "Epoch [70/100], d_loss: 0.0081, g_loss: 6.3873\n",
      "Epoch [71/100], d_loss: 0.0224, g_loss: 5.5711\n",
      "Epoch [72/100], d_loss: 0.0063, g_loss: 6.6697\n",
      "Epoch [73/100], d_loss: 0.0152, g_loss: 5.8203\n",
      "Epoch [74/100], d_loss: 0.0365, g_loss: 5.1408\n",
      "Epoch [75/100], d_loss: 0.1279, g_loss: 3.8203\n",
      "Epoch [76/100], d_loss: 0.0584, g_loss: 4.0555\n",
      "Epoch [77/100], d_loss: 0.1385, g_loss: 6.2629\n",
      "Epoch [78/100], d_loss: 0.0431, g_loss: 6.6427\n",
      "Epoch [79/100], d_loss: 0.0477, g_loss: 6.3119\n",
      "Epoch [80/100], d_loss: 0.0417, g_loss: 4.2970\n",
      "Epoch [81/100], d_loss: 0.0068, g_loss: 9.6457\n",
      "Epoch [82/100], d_loss: 0.0303, g_loss: 5.5448\n",
      "Epoch [83/100], d_loss: 0.2379, g_loss: 3.3892\n",
      "Epoch [84/100], d_loss: 0.0751, g_loss: 5.7381\n",
      "Epoch [85/100], d_loss: 0.0405, g_loss: 5.3706\n",
      "Epoch [86/100], d_loss: 0.0186, g_loss: 5.7084\n",
      "Epoch [87/100], d_loss: 0.0326, g_loss: 5.2462\n",
      "Epoch [88/100], d_loss: 0.0644, g_loss: 4.4193\n",
      "Epoch [89/100], d_loss: 0.2950, g_loss: 6.1873\n",
      "Epoch [90/100], d_loss: 0.0219, g_loss: 5.7007\n",
      "Epoch [91/100], d_loss: 0.0207, g_loss: 5.9963\n",
      "Epoch [92/100], d_loss: 0.0138, g_loss: 6.0400\n",
      "Epoch [93/100], d_loss: 0.0094, g_loss: 5.9995\n",
      "Epoch [94/100], d_loss: 0.0150, g_loss: 6.2365\n",
      "Epoch [95/100], d_loss: 0.0405, g_loss: 6.1186\n",
      "Epoch [96/100], d_loss: 0.0222, g_loss: 6.0048\n",
      "Epoch [97/100], d_loss: 0.0041, g_loss: 3.6016\n",
      "Epoch [98/100], d_loss: 0.2016, g_loss: 4.7785\n",
      "Epoch [99/100], d_loss: 0.1141, g_loss: 4.0861\n",
      "Epoch [100/100], d_loss: 0.9542, g_loss: 1.2892\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:16:37.526627Z",
     "start_time": "2024-10-17T12:16:37.507627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    \n",
    "    # Generate new signatures\n",
    "vae_samples = generate_vae_signatures(vae)\n",
    "gan_samples = generate_gan_signatures(generator)\n",
    "    \n",
    "    # Save generated samples\n",
    "save_image(vae_samples, 'vae_generated_signatures.png')\n",
    "save_image(gan_samples, 'gan_generated_signatures.png')\n",
    "    \n",
    "    # Evaluate models\n",
    "vae_loss = evaluate_model(vae, test_loader)\n",
    "print(f'VAE Test Loss: {vae_loss:.4f}')\n",
    "\n",
    "# Generate group-specific signatures (placeholder)\n",
    "num_groups = len(train_dataset.dataset.classes)\n",
    "for group_idx in range(num_groups):\n",
    "    group_name = train_dataset.dataset.classes[group_idx]\n",
    "    vae_group_samples = generate_vae_signatures(vae, num_samples=5, group_idx=group_idx)\n",
    "    gan_group_samples = generate_gan_signatures(generator, num_samples=5, group_idx=group_idx)\n",
    "    save_image(vae_group_samples, f'vae_generated_signatures_group_{group_name}.png')\n",
    "    save_image(gan_group_samples, f'gan_generated_signatures_group_{group_name}.png')\n",
    "\n",
    "print(\"Training, evaluation, and group-specific generation complete!\")"
   ],
   "id": "44ec3cc2a1b48d29",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 15 (3129987277.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[25], line 16\u001B[1;36m\u001B[0m\n\u001B[1;33m    group_name = train_dataset.dataset.classes[group_idx]\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block after 'for' statement on line 15\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
