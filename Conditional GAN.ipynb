{
 "cells": [
  {
   "cell_type": "code",
   "id": "92230aa24b548ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:42.226463Z",
     "start_time": "2024-10-17T15:13:32.327185Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:42.365641Z",
     "start_time": "2024-10-17T15:13:42.249276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "911bba05615a3a51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.732805Z",
     "start_time": "2024-10-17T15:13:43.719689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Updated Data Loading and Preprocessing\n",
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.images.append(os.path.join(class_path, img_name))\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "id": "fd845afd203480b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.763887Z",
     "start_time": "2024-10-17T15:13:43.754882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load and split the dataset\n",
    "def load_and_split_data(root_dir, train_ratio=0.8):\n",
    "    full_dataset = SignatureDataset(root_dir=root_dir, transform=transform)\n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset"
   ],
   "id": "ec9c6b5751ec11eb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.842464Z",
     "start_time": "2024-10-17T15:13:43.831463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VAE Implementation (unchanged)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ],
   "id": "3e82333b816d7c41",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.920219Z",
     "start_time": "2024-10-17T15:13:43.908751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GAN Implementation (unchanged)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "76ae5edd702917b1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.967669Z",
     "start_time": "2024-10-17T15:13:43.953816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training functions (slightly modified)\n",
    "def train_vae(vae, optimizer, train_loader, num_epochs=100):\n",
    "    vae.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch, _ in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(batch)\n",
    "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def train_gan(generator, discriminator, g_optimizer, d_optimizer, train_loader, num_epochs=100):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, _ in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            batch_size = batch.size(0)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            outputs = discriminator(batch)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "\n",
    "            noise = torch.randn(batch_size, generator.latent_dim).to(device)\n",
    "            fake_images = generator(noise)\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ],
   "id": "28551e11df72cfd7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:43.999726Z",
     "start_time": "2024-10-17T15:13:43.983729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generating New Signatures (updated for group-based generation)\n",
    "def generate_vae_signatures(vae, num_samples=10, group_idx=None):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        if group_idx is not None:\n",
    "            # Implement group-specific latent space sampling\n",
    "            z = torch.randn(num_samples, vae.latent_dim).to(device) + group_idx\n",
    "        else:\n",
    "            z = torch.randn(num_samples, vae.latent_dim).to(device)\n",
    "        samples = vae.decode(z)\n",
    "    return samples\n",
    "\n",
    "def generate_gan_signatures(generator, num_samples=10, group_idx=None):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, generator.latent_dim).to(device)\n",
    "        if group_idx is not None:\n",
    "            z = noise + group_idx\n",
    "        else:\n",
    "            z = noise\n",
    "        samples = generator(noise)\n",
    "    return samples\n",
    "\n",
    "# Metrics and Evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, _ in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ],
   "id": "3dd45d00b0e75760",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:48:06.752095Z",
     "start_time": "2024-10-17T15:48:06.748301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latent_dim = 500\n",
    "root_dir = 'Grouped_Output'  "
   ],
   "id": "39da436e9150084b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:13:44.078946Z",
     "start_time": "2024-10-17T15:13:44.048890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and split the data\n",
    "train_dataset, test_dataset = load_and_split_data(root_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)   "
   ],
   "id": "fc3ebc80d11f0634",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:21:56.448510Z",
     "start_time": "2024-10-17T15:13:44.097916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VAE training\n",
    "vae = VAE(latent_dim).to(device)\n",
    "vae_optimizer = optim.Adam(vae.parameters())\n",
    "train_vae(vae, vae_optimizer, train_loader,num_epochs=1500)"
   ],
   "id": "d1c1c9ca774caa0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 161871.2263\n",
      "Epoch 2, Loss: 158686.8174\n",
      "Epoch 3, Loss: 157970.8489\n",
      "Epoch 4, Loss: 157717.4922\n",
      "Epoch 5, Loss: 157427.9253\n",
      "Epoch 6, Loss: 157091.2727\n",
      "Epoch 7, Loss: 156880.4265\n",
      "Epoch 8, Loss: 156467.2346\n",
      "Epoch 9, Loss: 156336.6587\n",
      "Epoch 10, Loss: 156274.1013\n",
      "Epoch 11, Loss: 156008.1587\n",
      "Epoch 12, Loss: 155946.6541\n",
      "Epoch 13, Loss: 155879.2517\n",
      "Epoch 14, Loss: 154434.5994\n",
      "Epoch 15, Loss: 152120.7046\n",
      "Epoch 16, Loss: 149678.5898\n",
      "Epoch 17, Loss: 147898.2937\n",
      "Epoch 18, Loss: 145935.8523\n",
      "Epoch 19, Loss: 144906.4243\n",
      "Epoch 20, Loss: 144947.3774\n",
      "Epoch 21, Loss: 144857.1812\n",
      "Epoch 22, Loss: 144265.2004\n",
      "Epoch 23, Loss: 143829.6934\n",
      "Epoch 24, Loss: 144609.1226\n",
      "Epoch 25, Loss: 143458.5984\n",
      "Epoch 26, Loss: 143706.9707\n",
      "Epoch 27, Loss: 143463.1028\n",
      "Epoch 28, Loss: 143047.3708\n",
      "Epoch 29, Loss: 142835.1455\n",
      "Epoch 30, Loss: 142861.5154\n",
      "Epoch 31, Loss: 143585.5120\n",
      "Epoch 32, Loss: 143580.1409\n",
      "Epoch 33, Loss: 142402.0161\n",
      "Epoch 34, Loss: 142572.9077\n",
      "Epoch 35, Loss: 143058.9792\n",
      "Epoch 36, Loss: 142968.9067\n",
      "Epoch 37, Loss: 142923.3914\n",
      "Epoch 38, Loss: 142619.3110\n",
      "Epoch 39, Loss: 142605.2998\n",
      "Epoch 40, Loss: 142450.6018\n",
      "Epoch 41, Loss: 143409.5054\n",
      "Epoch 42, Loss: 142513.1682\n",
      "Epoch 43, Loss: 143037.3962\n",
      "Epoch 44, Loss: 142304.6443\n",
      "Epoch 45, Loss: 142053.3567\n",
      "Epoch 46, Loss: 142449.6021\n",
      "Epoch 47, Loss: 142206.2637\n",
      "Epoch 48, Loss: 143299.7463\n",
      "Epoch 49, Loss: 142987.0854\n",
      "Epoch 50, Loss: 141980.1606\n",
      "Epoch 51, Loss: 142937.4524\n",
      "Epoch 52, Loss: 142638.1460\n",
      "Epoch 53, Loss: 142258.5049\n",
      "Epoch 54, Loss: 141903.8914\n",
      "Epoch 55, Loss: 142392.3909\n",
      "Epoch 56, Loss: 142362.4023\n",
      "Epoch 57, Loss: 142391.1680\n",
      "Epoch 58, Loss: 142878.0183\n",
      "Epoch 59, Loss: 141707.1113\n",
      "Epoch 60, Loss: 141593.8550\n",
      "Epoch 61, Loss: 142561.7769\n",
      "Epoch 62, Loss: 142595.9004\n",
      "Epoch 63, Loss: 142482.2175\n",
      "Epoch 64, Loss: 142991.7488\n",
      "Epoch 65, Loss: 143339.3972\n",
      "Epoch 66, Loss: 142327.1848\n",
      "Epoch 67, Loss: 142135.7463\n",
      "Epoch 68, Loss: 142699.4568\n",
      "Epoch 69, Loss: 141670.0725\n",
      "Epoch 70, Loss: 142110.3958\n",
      "Epoch 71, Loss: 142487.2649\n",
      "Epoch 72, Loss: 142669.8105\n",
      "Epoch 73, Loss: 141749.7710\n",
      "Epoch 74, Loss: 142436.2317\n",
      "Epoch 75, Loss: 141935.1560\n",
      "Epoch 76, Loss: 142055.4436\n",
      "Epoch 77, Loss: 142293.5273\n",
      "Epoch 78, Loss: 141831.3660\n",
      "Epoch 79, Loss: 141945.7822\n",
      "Epoch 80, Loss: 142082.1777\n",
      "Epoch 81, Loss: 142402.3140\n",
      "Epoch 82, Loss: 142263.7727\n",
      "Epoch 83, Loss: 142873.3640\n",
      "Epoch 84, Loss: 141880.7085\n",
      "Epoch 85, Loss: 141465.9395\n",
      "Epoch 86, Loss: 142289.4912\n",
      "Epoch 87, Loss: 141816.6152\n",
      "Epoch 88, Loss: 141842.9014\n",
      "Epoch 89, Loss: 141783.2749\n",
      "Epoch 90, Loss: 141648.2681\n",
      "Epoch 91, Loss: 141550.1836\n",
      "Epoch 92, Loss: 141562.3811\n",
      "Epoch 93, Loss: 142007.0127\n",
      "Epoch 94, Loss: 141518.8813\n",
      "Epoch 95, Loss: 142001.2695\n",
      "Epoch 96, Loss: 141701.8184\n",
      "Epoch 97, Loss: 142119.7471\n",
      "Epoch 98, Loss: 141890.0247\n",
      "Epoch 99, Loss: 141947.6768\n",
      "Epoch 100, Loss: 141880.8760\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:28:37.804569Z",
     "start_time": "2024-10-17T15:21:56.489368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GAN training\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "train_gan(generator, discriminator, g_optimizer, d_optimizer, train_loader,num_epochs=2500)"
   ],
   "id": "201a124c4bf17e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], d_loss: 0.1232, g_loss: 3.7374\n",
      "Epoch [2/100], d_loss: 0.0535, g_loss: 4.8415\n",
      "Epoch [3/100], d_loss: 0.0260, g_loss: 5.3070\n",
      "Epoch [4/100], d_loss: 0.0150, g_loss: 5.9209\n",
      "Epoch [5/100], d_loss: 0.0111, g_loss: 6.3410\n",
      "Epoch [6/100], d_loss: 0.0084, g_loss: 6.3041\n",
      "Epoch [7/100], d_loss: 0.0060, g_loss: 6.5305\n",
      "Epoch [8/100], d_loss: 0.0066, g_loss: 6.6569\n",
      "Epoch [9/100], d_loss: 0.0054, g_loss: 6.8628\n",
      "Epoch [10/100], d_loss: 0.0089, g_loss: 6.8088\n",
      "Epoch [11/100], d_loss: 0.0054, g_loss: 6.9921\n",
      "Epoch [12/100], d_loss: 0.0041, g_loss: 6.8431\n",
      "Epoch [13/100], d_loss: 0.0038, g_loss: 7.0618\n",
      "Epoch [14/100], d_loss: 0.0041, g_loss: 7.1070\n",
      "Epoch [15/100], d_loss: 0.0048, g_loss: 7.1594\n",
      "Epoch [16/100], d_loss: 0.0034, g_loss: 7.3429\n",
      "Epoch [17/100], d_loss: 0.0023, g_loss: 7.4777\n",
      "Epoch [18/100], d_loss: 0.0060, g_loss: 6.7932\n",
      "Epoch [19/100], d_loss: 0.0016, g_loss: 7.8731\n",
      "Epoch [20/100], d_loss: 0.0022, g_loss: 7.9461\n",
      "Epoch [21/100], d_loss: 0.0062, g_loss: 15.0403\n",
      "Epoch [22/100], d_loss: 0.4536, g_loss: 13.5683\n",
      "Epoch [23/100], d_loss: 0.0017, g_loss: 7.7836\n",
      "Epoch [24/100], d_loss: 0.5504, g_loss: 9.2182\n",
      "Epoch [25/100], d_loss: 0.0701, g_loss: 6.1255\n",
      "Epoch [26/100], d_loss: 0.0482, g_loss: 6.6678\n",
      "Epoch [27/100], d_loss: 2.5890, g_loss: 7.7951\n",
      "Epoch [28/100], d_loss: 0.3368, g_loss: 4.7317\n",
      "Epoch [29/100], d_loss: 0.2354, g_loss: 5.5465\n",
      "Epoch [30/100], d_loss: 0.3086, g_loss: 6.9653\n",
      "Epoch [31/100], d_loss: 0.1220, g_loss: 5.0592\n",
      "Epoch [32/100], d_loss: 0.0772, g_loss: 5.8817\n",
      "Epoch [33/100], d_loss: 0.0817, g_loss: 8.1008\n",
      "Epoch [34/100], d_loss: 0.2059, g_loss: 5.8132\n",
      "Epoch [35/100], d_loss: 0.4714, g_loss: 9.3409\n",
      "Epoch [36/100], d_loss: 0.2723, g_loss: 5.6880\n",
      "Epoch [37/100], d_loss: 0.1074, g_loss: 3.8769\n",
      "Epoch [38/100], d_loss: 0.0900, g_loss: 4.4029\n",
      "Epoch [39/100], d_loss: 0.1144, g_loss: 5.7152\n",
      "Epoch [40/100], d_loss: 0.1302, g_loss: 6.8029\n",
      "Epoch [41/100], d_loss: 0.0860, g_loss: 5.5111\n",
      "Epoch [42/100], d_loss: 4.2352, g_loss: 12.5978\n",
      "Epoch [43/100], d_loss: 0.4498, g_loss: 5.3109\n",
      "Epoch [44/100], d_loss: 0.1109, g_loss: 4.8418\n",
      "Epoch [45/100], d_loss: 0.0961, g_loss: 4.5265\n",
      "Epoch [46/100], d_loss: 0.1146, g_loss: 4.4435\n",
      "Epoch [47/100], d_loss: 0.0807, g_loss: 4.3790\n",
      "Epoch [48/100], d_loss: 0.0691, g_loss: 5.1083\n",
      "Epoch [49/100], d_loss: 0.0726, g_loss: 4.7457\n",
      "Epoch [50/100], d_loss: 0.0467, g_loss: 4.8718\n",
      "Epoch [51/100], d_loss: 0.0254, g_loss: 5.4939\n",
      "Epoch [52/100], d_loss: 0.0187, g_loss: 5.3791\n",
      "Epoch [53/100], d_loss: 0.0217, g_loss: 5.6682\n",
      "Epoch [54/100], d_loss: 0.0163, g_loss: 6.0809\n",
      "Epoch [55/100], d_loss: 0.0306, g_loss: 6.0120\n",
      "Epoch [56/100], d_loss: 0.0135, g_loss: 5.9954\n",
      "Epoch [57/100], d_loss: 0.0197, g_loss: 5.9489\n",
      "Epoch [58/100], d_loss: 0.0168, g_loss: 5.6410\n",
      "Epoch [59/100], d_loss: 0.0116, g_loss: 5.6305\n",
      "Epoch [60/100], d_loss: 0.0203, g_loss: 5.9079\n",
      "Epoch [61/100], d_loss: 0.0132, g_loss: 6.0009\n",
      "Epoch [62/100], d_loss: 0.0081, g_loss: 6.1745\n",
      "Epoch [63/100], d_loss: 0.0098, g_loss: 6.0358\n",
      "Epoch [64/100], d_loss: 0.0098, g_loss: 5.9239\n",
      "Epoch [65/100], d_loss: 0.0081, g_loss: 5.8602\n",
      "Epoch [66/100], d_loss: 0.0068, g_loss: 5.9673\n",
      "Epoch [67/100], d_loss: 0.0080, g_loss: 6.1664\n",
      "Epoch [68/100], d_loss: 0.0180, g_loss: 6.0592\n",
      "Epoch [69/100], d_loss: 0.0137, g_loss: 6.1230\n",
      "Epoch [70/100], d_loss: 0.0201, g_loss: 6.3622\n",
      "Epoch [71/100], d_loss: 0.0086, g_loss: 7.2645\n",
      "Epoch [72/100], d_loss: 0.0138, g_loss: 6.0297\n",
      "Epoch [73/100], d_loss: 0.0134, g_loss: 5.9718\n",
      "Epoch [74/100], d_loss: 0.0323, g_loss: 6.0432\n",
      "Epoch [75/100], d_loss: 0.0134, g_loss: 4.7719\n",
      "Epoch [76/100], d_loss: 0.2794, g_loss: 16.0901\n",
      "Epoch [77/100], d_loss: 0.0146, g_loss: 5.4763\n",
      "Epoch [78/100], d_loss: 0.0457, g_loss: 5.2591\n",
      "Epoch [79/100], d_loss: 0.4090, g_loss: 3.5764\n",
      "Epoch [80/100], d_loss: 0.0483, g_loss: 6.3379\n",
      "Epoch [81/100], d_loss: 0.1162, g_loss: 5.1398\n",
      "Epoch [82/100], d_loss: 0.0738, g_loss: 5.8477\n",
      "Epoch [83/100], d_loss: 0.0574, g_loss: 5.8517\n",
      "Epoch [84/100], d_loss: 0.1007, g_loss: 6.3323\n",
      "Epoch [85/100], d_loss: 0.0646, g_loss: 4.8189\n",
      "Epoch [86/100], d_loss: 0.0486, g_loss: 5.1668\n",
      "Epoch [87/100], d_loss: 0.0423, g_loss: 5.0167\n",
      "Epoch [88/100], d_loss: 0.0878, g_loss: 6.1207\n",
      "Epoch [89/100], d_loss: 0.1033, g_loss: 5.9859\n",
      "Epoch [90/100], d_loss: 0.0384, g_loss: 3.3992\n",
      "Epoch [91/100], d_loss: 0.0185, g_loss: 5.0603\n",
      "Epoch [92/100], d_loss: 0.0179, g_loss: 4.7093\n",
      "Epoch [93/100], d_loss: 0.0375, g_loss: 5.2320\n",
      "Epoch [94/100], d_loss: 0.0200, g_loss: 5.1640\n",
      "Epoch [95/100], d_loss: 0.0197, g_loss: 5.2621\n",
      "Epoch [96/100], d_loss: 0.0299, g_loss: 5.3337\n",
      "Epoch [97/100], d_loss: 0.0883, g_loss: 8.3914\n",
      "Epoch [98/100], d_loss: 0.2676, g_loss: 2.0559\n",
      "Epoch [99/100], d_loss: 0.0210, g_loss: 5.7751\n",
      "Epoch [100/100], d_loss: 0.0959, g_loss: 5.2296\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:28:37.929666Z",
     "start_time": "2024-10-17T15:28:37.871751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate new signatures\n",
    "vae_samples = generate_vae_signatures(vae)\n",
    "gan_samples = generate_gan_signatures(generator)"
   ],
   "id": "c4b3e39c8558094b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:28:38.084147Z",
     "start_time": "2024-10-17T15:28:37.975896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save generated samples\n",
    "save_image(vae_samples, 'Output/vae_generated_signatures.png')\n",
    "save_image(gan_samples, 'Output/gan_generated_signatures.png')"
   ],
   "id": "9d48f92b4d051ba4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:28:39.845193Z",
     "start_time": "2024-10-17T15:28:38.337489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate models\n",
    "vae_loss = evaluate_model(vae, test_loader)\n",
    "print(f'VAE Test Loss: {vae_loss:.4f}')"
   ],
   "id": "917d333c7b1c3b5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Test Loss: 141480.6172\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:47:11.825250Z",
     "start_time": "2024-10-17T15:47:10.515072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate group-specific signatures (placeholder)\n",
    "num_groups = len(train_dataset.dataset.classes)\n",
    "for group_idx in range(num_groups):\n",
    "    group_name = train_dataset.dataset.classes[group_idx]\n",
    "    vae_group_samples = generate_vae_signatures(vae, num_samples=1, group_idx=group_idx)\n",
    "    gan_group_samples = generate_gan_signatures(generator, num_samples=1, group_idx=group_idx)\n",
    "    os.makedirs(\"Output/\"+group_name, exist_ok=True)\n",
    "    save_image(vae_group_samples, f'Output/{group_name}/vae_generated_signatures_group_{group_name}.png')\n",
    "    save_image(gan_group_samples, f'Output/{group_name}/gan_generated_signatures_group_{group_name}.png')\n",
    "\n",
    "print(\"Training, evaluation, and group-specific generation complete!\")"
   ],
   "id": "e26ad86dd96152ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, evaluation, and group-specific generation complete!\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
