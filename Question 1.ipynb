{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.cuda import device\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33b2b6f721868ad",
   "metadata": {},
   "source": [
    "# reading all images from directory\n",
    "Images = {}\n",
    "Folder = 'Grouped_Output'\n",
    "for filename in os.listdir(Folder):\n",
    "    # opening all folders in this directory and saving the images in a dictionary\n",
    "    Images[filename] = []\n",
    "    for img in os.listdir(Folder + '/' + filename):\n",
    "        Images[filename].append(img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d54c63cd76e0269",
   "metadata": {},
   "source": [
    "# # printing one image from each folder\n",
    "# for folder in Images.keys():\n",
    "#     img = Image.open(Folder + '/' + folder + '/' + Images[folder][0])\n",
    "#     plt.imshow(np.array(img))\n",
    "#     plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ab6d0b48f52581d",
   "metadata": {},
   "source": [
    "# Pre-processing the images \n",
    "# 1. Resizing the images to 64x64\n",
    "# 2. Normalizing the images\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b099ae4f0ea7fc7",
   "metadata": {},
   "source": [
    "# applying transformation to all images\n",
    "Images_tensor = {}\n",
    "for folder in Images.keys():\n",
    "    Images_tensor[folder] = []\n",
    "    for img in Images[folder]:\n",
    "        img = Image.open(Folder + '/' + folder + '/' + img)\n",
    "        img = transform(img)\n",
    "        Images_tensor[folder].append(img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1e5a22ebabcc435",
   "metadata": {},
   "source": [
    "## VAE Implementation:  \n",
    "1. Define the encoder and decoder networks.\n",
    "2. Define the VAE loss function (reconstruction loss + KL divergence).\n",
    "3. Train the VAE on the augmented dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "fcf39c98d4f4ad3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ],
   "id": "53c7277947160059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 256, 4, 4)\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = torch.sigmoid(self.deconv4(z))\n",
    "        return z"
   ],
   "id": "82643fb61b9a477c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ],
   "id": "4a18cf9384b257c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ],
   "id": "8ca81ec1ae76e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training loop\n",
    "vae = VAE(latent_dim=20)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Function to check tensor dimensions\n",
    "def print_tensor_info(tensor, name):\n",
    "    print(f\"{name} shape: {tensor.shape}\")\n",
    "    print(f\"{name} size: {tensor.numel()}\")\n",
    "    print(f\"{name} dtype: {tensor.dtype}\")\n",
    "    print(f\"{name} device: {tensor.device}\")\n",
    "    print(f\"{name} requires_grad: {tensor.requires_grad}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Preprocess images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "id": "830ffbb9959ca49e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply transformation to all images\n",
    "Images_tensor = {}\n",
    "for folder in Images.keys():\n",
    "    Images_tensor[folder] = []\n",
    "    for img_path in Images[folder]:\n",
    "        img = Image.open(os.path.join(Folder, folder, img_path))\n",
    "        img = transform(img)\n",
    "        Images_tensor[folder].append(img)"
   ],
   "id": "bcb417b460af423a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "epoch_Number = 10\n",
    "for epoch in range(epoch_Number):\n",
    "    for folder in Images_tensor.keys():\n",
    "        for img in Images_tensor[folder]:\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            print_tensor_info(img, \"Input image\")  # Debug info\n",
    "            recon_img, mu, logvar = vae(img)\n",
    "            loss = vae_loss(recon_img, img, mu, logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ],
   "id": "24e14785fe56b06d"
  },
  {
   "cell_type": "code",
   "id": "37e38508db18b874",
   "metadata": {},
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 64*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(64*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93b9e15226914a0d",
   "metadata": {},
   "source": [
    "# Training\n",
    "epoch_Number = 10\n",
    "for epoch in range(epoch_Number):\n",
    "    for folder in Images_tensor.keys():\n",
    "        for img in Images_tensor[folder]:\n",
    "            img = img.view(-1, 64*64)\n",
    "            z = torch.randn(1, 100)\n",
    "            fake_img = generator(z)\n",
    "            real_label = torch.ones(1, 1)\n",
    "            fake_label = torch.zeros(1, 1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_output = discriminator(img)\n",
    "            fake_output = discriminator(fake_img)\n",
    "            real_loss = adversarial_loss(real_output, real_label)\n",
    "            fake_loss = adversarial_loss(fake_output, fake_label)\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(1, 100)\n",
    "            fake_img = generator(z)\n",
    "            output = discriminator(fake_img)\n",
    "            g_loss = adversarial_loss(output, real_label)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "    print(f'Epoch: {epoch}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1a78b9a83446251",
   "metadata": {},
   "source": [
    "# Generating images\n",
    "z = torch.randn(1, 100)\n",
    "fake_img = generator(z)\n",
    "fake_img = fake_img.view(64, 64)\n",
    "plt.imshow(fake_img.detach().numpy())\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# saving the model\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')"
   ],
   "id": "277dd3387e0c5741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# genreating image from group 7\n",
    "z = torch.randn(1, 100)\n",
    "fake_img = generator(z)\n",
    "fake_img = fake_img.view(64, 64)\n",
    "# saving the image\n",
    "plt.imshow(fake_img.detach().numpy())\n",
    "plt.savefig('Generated_Image.png')"
   ],
   "id": "c8aa1e4e3cf36866",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
