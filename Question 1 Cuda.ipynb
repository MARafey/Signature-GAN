{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:03:34.099376Z",
     "start_time": "2024-10-17T10:03:34.091330Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:03:34.131336Z",
     "start_time": "2024-10-17T10:03:34.125757Z"
    }
   },
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:03:34.271863Z",
     "start_time": "2024-10-17T10:03:34.241059Z"
    }
   },
   "source": [
    "# Reading all images from directory\n",
    "Images = {}\n",
    "Folder = 'Grouped_Output'\n",
    "group_to_id = {}\n",
    "for i, filename in enumerate(os.listdir(Folder)):\n",
    "    Images[filename] = []\n",
    "    group_to_id[filename] = i\n",
    "    for img in os.listdir(Folder + '/' + filename):\n",
    "        Images[filename].append(img)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.631561Z",
     "start_time": "2024-10-17T10:03:34.336897Z"
    }
   },
   "source": [
    "# Pre-processing the images by adding rotation and some noise to copy if the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Applying transformation to all images\n",
    "Images_tensor = {}\n",
    "for folder in Images.keys():\n",
    "    Images_tensor[folder] = []\n",
    "    for img in Images[folder]:\n",
    "        img = Image.open(Folder + '/' + folder + '/' + img)\n",
    "        img = transform(img).to(device)\n",
    "        Images_tensor[folder].append(img)\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.647757Z",
     "start_time": "2024-10-17T10:04:12.636612Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_groups, 256 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z, group):\n",
    "        z = torch.cat([z, group], dim=1)\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 256, 4, 4)\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = torch.sigmoid(self.deconv4(z))\n",
    "        return z\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.710796Z",
     "start_time": "2024-10-17T10:04:12.696336Z"
    }
   },
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, num_groups)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, group):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        group = group.unsqueeze(0).expand(z.size(0), -1)\n",
    "        return self.decoder(z, group), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.757300Z",
     "start_time": "2024-10-17T10:04:12.742322Z"
    }
   },
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_groups, 256 * 4 * 4)  # Fully connected layer to map latent vector\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)   # Output: 64x16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)    # Output: 32x32x32\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)     # Output: 3x64x64\n",
    "\n",
    "    def forward(self, z, group):\n",
    "        group = group.unsqueeze(0).expand(z.size(0), -1)  # Add group info to latent vector\n",
    "        x = torch.cat([z, group], dim=1)\n",
    "        x = F.relu(self.fc(x))  # Fully connected layer\n",
    "        x = x.view(x.size(0), 256, 4, 4)  # Reshape to 256x4x4 for ConvTranspose2d\n",
    "        \n",
    "        x = F.relu(self.deconv1(x))  # 128x8x8\n",
    "        x = F.relu(self.deconv2(x))  # 64x16x16\n",
    "        x = F.relu(self.deconv3(x))  # 32x32x32\n",
    "        x = torch.tanh(self.deconv4(x))  # 3x64x64 (tanh is often used to keep values between -1 and 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_groups):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3 + num_groups, 32, kernel_size=4, stride=2, padding=1)  # Input: 3+num_groups channels, Output: 32x32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x8x8\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # Output: 256x4x4\n",
    "\n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)  # Fully connected layer for binary classification\n",
    "\n",
    "    def forward(self, x, group):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        group = group.view(batch_size, -1, 1, 1)  # Reshape group embedding\n",
    "        group = group.expand(batch_size, -1, height, width)  # Expand to match image size\n",
    "        x = torch.cat([x, group], dim=1)  # Concatenate image and group embedding\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)  # 32x32x32\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)  # 64x16x16\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)  # 128x8x8\n",
    "        x = F.leaky_relu(self.conv4(x), 0.2)  # 256x4x4\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc(x))  # Binary output (real or fake)\n",
    "\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.851220Z",
     "start_time": "2024-10-17T10:04:12.790309Z"
    }
   },
   "source": [
    "# Initialize models\n",
    "latent_dim = 20\n",
    "vae = VAE(latent_dim=latent_dim, num_groups=num_groups).to(device)\n",
    "generator = Generator(latent_dim, num_groups).to(device)\n",
    "discriminator = Discriminator(num_groups).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.912974Z",
     "start_time": "2024-10-17T10:04:12.898918Z"
    }
   },
   "source": [
    "def train_gan(num_epochs, switch_epoch_G, switch_epoch_D, joint_train_start_epoch):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        num_batches = 0\n",
    "        for folder in Images_tensor.keys():\n",
    "            group_id = group_to_id[folder]\n",
    "            group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "            for img in Images_tensor[folder]:\n",
    "                # Ensure img is a 4D tensor: (batch_size, channels, height, width)\n",
    "                img = img.unsqueeze(0) if img.dim() == 3 else img\n",
    "                batch_size = img.size(0)\n",
    "                # img = img.view(-1, 64*64*3)\n",
    "                # batch_size = img.size(0)\n",
    "                \n",
    "                # Train only Discriminator for specified epochs\n",
    "                if epoch < switch_epoch_D or (epoch >= joint_train_start_epoch and epoch % 2 == 0):\n",
    "                    real_label = torch.ones(batch_size, 1).to(device)\n",
    "                    fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "                    \n",
    "                    optimizer_D.zero_grad()\n",
    "                    \n",
    "                    real_output = discriminator(img, group_onehot)\n",
    "                    d_loss_real = adversarial_loss(real_output, real_label)\n",
    "                    \n",
    "                    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                    fake_img = generator(z, group_onehot)\n",
    "                    fake_output = discriminator(fake_img.detach(), group_onehot)\n",
    "                    d_loss_fake = adversarial_loss(fake_output, fake_label)\n",
    "                    \n",
    "                    d_loss = d_loss_real + d_loss_fake\n",
    "                    d_loss.backward()\n",
    "                    optimizer_D.step()\n",
    "                    \n",
    "                    total_d_loss += d_loss.item()\n",
    "\n",
    "                # Train only Generator for specified epochs\n",
    "                if epoch >= switch_epoch_G or (epoch >= joint_train_start_epoch and epoch % 2 != 0):\n",
    "            \n",
    "                    optimizer_G.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                    fake_img = generator(z, group_onehot)\n",
    "                    output = discriminator(fake_img, group_onehot)\n",
    "                    g_loss = adversarial_loss(output, real_label)\n",
    "                    g_loss.backward()\n",
    "                    optimizer_G.step()\n",
    "\n",
    "                    total_g_loss += g_loss.item()\n",
    "                \n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_d_loss = total_d_loss / num_batches if total_d_loss > 0 else 0\n",
    "        avg_g_loss = total_g_loss / num_batches if total_g_loss > 0 else 0\n",
    "        print(f'Epoch {epoch+1}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}')"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:12.974585Z",
     "start_time": "2024-10-17T10:04:12.960467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_vae(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for folder in Images_tensor.keys():\n",
    "            group_id = group_to_id[folder]\n",
    "            group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "            for img in Images_tensor[folder]:\n",
    "                img = img.unsqueeze(0)  # Add batch dimension\n",
    "                recon_img, mu, logvar = vae(img, group_onehot)\n",
    "                loss = vae_loss(recon_img, img, mu, logvar)\n",
    "                optimizer_vae.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_vae.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f'VAE Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}')\n"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:13.022433Z",
     "start_time": "2024-10-17T10:04:13.009434Z"
    }
   },
   "source": [
    "# Training\n",
    "vae_epochs = 2\n",
    "gan_epochs = 250"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T10:04:13.274309Z",
     "start_time": "2024-10-17T10:04:13.055940Z"
    }
   },
   "source": [
    "print(\"Training VAE...\")\n",
    "train_vae(vae_epochs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x654080 and 4096x20)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining VAE...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_vae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvae_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 10\u001B[0m, in \u001B[0;36mtrain_vae\u001B[1;34m(num_epochs)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m Images_tensor[folder]:\n\u001B[0;32m      9\u001B[0m     img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Add batch dimension\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     recon_img, mu, logvar \u001B[38;5;241m=\u001B[39m \u001B[43mvae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup_onehot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     loss \u001B[38;5;241m=\u001B[39m vae_loss(recon_img, img, mu, logvar)\n\u001B[0;32m     12\u001B[0m     optimizer_vae\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[76], line 14\u001B[0m, in \u001B[0;36mVAE.forward\u001B[1;34m(self, x, group)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, group):\n\u001B[1;32m---> 14\u001B[0m     mu, logvar \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameterize(mu, logvar)\n\u001B[0;32m     16\u001B[0m     group \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mexpand(z\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[75], line 17\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     15\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv4(x))\n\u001B[0;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m mu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc_mu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m logvar \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_logvar(x)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mu, logvar\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Semester 7\\Gen AI\\Assignment\\Assignment 2\\pythonProject\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x654080 and 4096x20)"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "switch_epoch_G = 5  # Train only Generator after epoch 5\n",
    "switch_epoch_D = 3  # Train only Discriminator after epoch 3\n",
    "joint_train_start_epoch = 10  # Start joint training from epoch 10\n",
    "print(\"Training GAN...\")\n",
    "train_gan(gan_epochs, switch_epoch_G, switch_epoch_D, joint_train_start_epoch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the models\n",
    "torch.save(vae.state_dict(), 'vae_model.pth')\n",
    "torch.save(generator.state_dict(), 'generator_model.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_model.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_signature(group_name):\n",
    "    if group_name not in group_to_id:\n",
    "        print(f\"Group '{group_name}' not found.\")\n",
    "        return None\n",
    "    \n",
    "    group_id = group_to_id[group_name]\n",
    "    group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "    \n",
    "    z = torch.randn(1, latent_dim).to(device)\n",
    "    fake_img = generator(z, group_onehot)\n",
    "    fake_img = fake_img.view(3, 64, 64).permute(1, 2, 0).cpu().detach()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(fake_img)\n",
    "    plt.title(f\"Generated Signature for Group: {group_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'Generated_Signature_{group_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Generated signature for group '{group_name}' saved as 'Generated_Signature_{group_name}.png'\")\n",
    "    return fake_img\n",
    "\n",
    "# Example usage:\n",
    "generate_signature(\"Group_11\")  # Replace \"Group1\" with the actual group name you want to generate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(group_to_id)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
