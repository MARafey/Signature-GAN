{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading all images from directory\n",
    "Images = {}\n",
    "Folder = 'Grouped_Output'\n",
    "group_to_id = {}\n",
    "for i, filename in enumerate(os.listdir(Folder)):\n",
    "    Images[filename] = []\n",
    "    group_to_id[filename] = i\n",
    "    for img in os.listdir(Folder + '/' + filename):\n",
    "        Images[filename].append(img)\n",
    "\n",
    "num_groups = len(group_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-processing the images\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "\n",
    "# Applying transformation to all images\n",
    "Images_tensor = {}\n",
    "for folder in Images.keys():\n",
    "    Images_tensor[folder] = []\n",
    "    for img in Images[folder]:\n",
    "        img = Image.open(Folder + '/' + folder + '/' + img)\n",
    "        img = transform(img).to(device)\n",
    "        Images_tensor[folder].append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_groups, 256 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z, group):\n",
    "        z = torch.cat([z, group], dim=1)\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 256, 4, 4)\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = torch.sigmoid(self.deconv4(z))\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, num_groups)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, group):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        group = group.unsqueeze(0).expand(z.size(0), -1)\n",
    "        return self.decoder(z, group), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_groups):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_groups, 256 * 4 * 4)  # Fully connected layer to map latent vector\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)   # Output: 64x16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)    # Output: 32x32x32\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)     # Output: 3x64x64\n",
    "\n",
    "    def forward(self, z, group):\n",
    "        group = group.unsqueeze(0).expand(z.size(0), -1)  # Add group info to latent vector\n",
    "        x = torch.cat([z, group], dim=1)\n",
    "        x = F.relu(self.fc(x))  # Fully connected layer\n",
    "        x = x.view(x.size(0), 256, 4, 4)  # Reshape to 256x4x4 for ConvTranspose2d\n",
    "        \n",
    "        x = F.relu(self.deconv1(x))  # 128x8x8\n",
    "        x = F.relu(self.deconv2(x))  # 64x16x16\n",
    "        x = F.relu(self.deconv3(x))  # 32x32x32\n",
    "        x = torch.tanh(self.deconv4(x))  # 3x64x64 (tanh is often used to keep values between -1 and 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_groups):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3 + num_groups, 32, kernel_size=4, stride=2, padding=1)  # Input: 3+num_groups channels, Output: 32x32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x16x16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x8x8\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # Output: 256x4x4\n",
    "        \n",
    "        self.fc = nn.Linear(256 * 4 * 4, 1)  # Fully connected layer for binary classification\n",
    "\n",
    "    def forward(self, x, group):\n",
    "        group = group.view(group.size(0), -1, 1, 1)  # Reshape group embedding to be broadcastable\n",
    "        group = group.expand(group.size(0), group.size(1), 64, 64)  # Expand to match image size (64x64)\n",
    "        x = torch.cat([x, group], dim=1)  # Concatenate image and group embedding along the channel dimension\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)  # 32x32x32\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)  # 64x16x16\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)  # 128x8x8\n",
    "        x = F.leaky_relu(self.conv4(x), 0.2)  # 256x4x4\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.sigmoid(self.fc(x))  # Binary output (real or fake)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "latent_dim = 20\n",
    "vae = VAE(latent_dim=latent_dim, num_groups=num_groups).to(device)\n",
    "generator = Generator(latent_dim, num_groups).to(device)\n",
    "discriminator = Discriminator(num_groups).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for folder in Images_tensor.keys():\n",
    "            group_id = group_to_id[folder]\n",
    "            group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "            for img in Images_tensor[folder]:\n",
    "                img = img.unsqueeze(0)  # Add batch dimension\n",
    "                recon_img, mu, logvar = vae(img, group_onehot)\n",
    "                loss = vae_loss(recon_img, img, mu, logvar)\n",
    "                optimizer_vae.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_vae.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f'VAE Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(num_epochs, switch_epoch_G, switch_epoch_D, joint_train_start_epoch):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        num_batches = 0\n",
    "        for folder in Images_tensor.keys():\n",
    "            group_id = group_to_id[folder]\n",
    "            group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "            for img in Images_tensor[folder]:\n",
    "                img = img.view(-1, 64*64*3)\n",
    "                batch_size = img.size(0)\n",
    "                \n",
    "                # Train only Discriminator for specified epochs\n",
    "                if epoch < switch_epoch_D or (epoch >= joint_train_start_epoch and epoch % 2 == 0):\n",
    "                    real_label = torch.ones(batch_size, 1).to(device)\n",
    "                    fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "                    \n",
    "                    optimizer_D.zero_grad()\n",
    "                    real_output = discriminator(img, group_onehot)\n",
    "                    d_loss_real = adversarial_loss(real_output, real_label)\n",
    "                    \n",
    "                    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                    fake_img = generator(z, group_onehot)\n",
    "                    fake_output = discriminator(fake_img.detach(), group_onehot)\n",
    "                    d_loss_fake = adversarial_loss(fake_output, fake_label)\n",
    "                    \n",
    "                    d_loss = d_loss_real + d_loss_fake\n",
    "                    d_loss.backward()\n",
    "                    optimizer_D.step()\n",
    "                    \n",
    "                    total_d_loss += d_loss.item()\n",
    "\n",
    "                # Train only Generator for specified epochs\n",
    "                if epoch >= switch_epoch_G or (epoch >= joint_train_start_epoch and epoch % 2 != 0):\n",
    "                    optimizer_G.zero_grad()\n",
    "                    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                    fake_img = generator(z, group_onehot)\n",
    "                    output = discriminator(fake_img, group_onehot)\n",
    "                    g_loss = adversarial_loss(output, real_label)\n",
    "                    g_loss.backward()\n",
    "                    optimizer_G.step()\n",
    "\n",
    "                    total_g_loss += g_loss.item()\n",
    "                \n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_d_loss = total_d_loss / num_batches if total_d_loss > 0 else 0\n",
    "        avg_g_loss = total_g_loss / num_batches if total_g_loss > 0 else 0\n",
    "        print(f'Epoch {epoch+1}, Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "vae_epochs = 25\n",
    "gan_epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n",
      "VAE Epoch: 1, Average Loss: 7967.4876\n",
      "VAE Epoch: 2, Average Loss: 7896.5509\n",
      "VAE Epoch: 3, Average Loss: 7849.9287\n",
      "VAE Epoch: 4, Average Loss: 7838.5224\n",
      "VAE Epoch: 5, Average Loss: 7823.0136\n",
      "VAE Epoch: 6, Average Loss: 7809.0817\n",
      "VAE Epoch: 7, Average Loss: 7799.2621\n",
      "VAE Epoch: 8, Average Loss: 7793.0351\n",
      "VAE Epoch: 9, Average Loss: 7788.5469\n",
      "VAE Epoch: 10, Average Loss: 7783.3222\n",
      "VAE Epoch: 11, Average Loss: 7780.1386\n",
      "VAE Epoch: 12, Average Loss: 7776.6827\n",
      "VAE Epoch: 13, Average Loss: 7774.8272\n",
      "VAE Epoch: 14, Average Loss: 7771.8794\n",
      "VAE Epoch: 15, Average Loss: 7769.4377\n",
      "VAE Epoch: 16, Average Loss: 7768.1334\n",
      "VAE Epoch: 17, Average Loss: 7766.2308\n",
      "VAE Epoch: 18, Average Loss: 7764.0874\n",
      "VAE Epoch: 19, Average Loss: 7762.7476\n",
      "VAE Epoch: 20, Average Loss: 7760.3775\n",
      "VAE Epoch: 21, Average Loss: 7758.7426\n",
      "VAE Epoch: 22, Average Loss: 7757.4540\n",
      "VAE Epoch: 23, Average Loss: 7755.2532\n",
      "VAE Epoch: 24, Average Loss: 7755.0627\n",
      "VAE Epoch: 25, Average Loss: 7752.5326\n"
     ]
    }
   ],
   "source": [
    "print(\"Training VAE...\")\n",
    "train_vae(vae_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m joint_train_start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Start joint training from epoch 10\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VAE...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_epoch_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_epoch_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_train_start_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 19\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(num_epochs, switch_epoch_G, switch_epoch_D, joint_train_start_epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m fake_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m adversarial_loss(real_output, real_label)\n\u001b[1;32m     22\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/Signature-GAN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Signature-GAN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 39\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x, group)\u001b[0m\n\u001b[1;32m     37\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mview(group\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape group embedding to be broadcastable\u001b[39;00m\n\u001b[1;32m     38\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39mexpand(group\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), group\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Expand to match image size (64x64)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Concatenate image and group embedding along the channel dimension\u001b[39;00m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x), \u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# 32x32x32\u001b[39;00m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x), \u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# 64x16x16\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 4"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "switch_epoch_G = 5  # Train only Generator after epoch 5\n",
    "switch_epoch_D = 3  # Train only Discriminator after epoch 3\n",
    "joint_train_start_epoch = 10  # Start joint training from epoch 10\n",
    "print(\"Training GAN...\")\n",
    "train_gan(gan_epochs, switch_epoch_G, switch_epoch_D, joint_train_start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(vae.state_dict(), 'vae_model.pth')\n",
    "torch.save(generator.state_dict(), 'generator_model.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signature(group_name):\n",
    "    if group_name not in group_to_id:\n",
    "        print(f\"Group '{group_name}' not found.\")\n",
    "        return None\n",
    "    \n",
    "    group_id = group_to_id[group_name]\n",
    "    group_onehot = F.one_hot(torch.tensor(group_id), num_classes=num_groups).float().to(device)\n",
    "    \n",
    "    z = torch.randn(1, latent_dim).to(device)\n",
    "    fake_img = generator(z, group_onehot)\n",
    "    fake_img = fake_img.view(3, 64, 64).permute(1, 2, 0).cpu().detach()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(fake_img)\n",
    "    plt.title(f\"Generated Signature for Group: {group_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'Generated_Signature_{group_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Generated signature for group '{group_name}' saved as 'Generated_Signature_{group_name}.png'\")\n",
    "    return fake_img\n",
    "\n",
    "# Example usage:\n",
    "generate_signature(\"Group_11\")  # Replace \"Group1\" with the actual group name you want to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_to_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
